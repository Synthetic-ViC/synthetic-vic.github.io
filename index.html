<!DOCTYPE html>
<html lang="en" >
<head>
  <meta charset="UTF-8">
  <title>Going Beyond Nouns With Vision & Language Models Using Synthetic Data</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css" media="screen,projection">
  <link rel="stylesheet" href="./style.css">
  <script src="https://code.jquery.com/jquery-3.2.1.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <!-- <script>
      document.addEventListener('DOMContentLoaded', function() {
      var elems = document.querySelectorAll('.materialboxed');
      var instances = M.Materialbox.init(elems, options);
      });
  </script> -->
</head>
<body class="section">
    <div class="section">
        <h3 class="header center black-text text-darken-4"><b>Going Beyond Nouns With Vision & Language</b> <br/><b>Models Using Synthetic Data</b></h3> 
        <h5 class="header center black-text text-darken-3">
            <a target="_blank" href="https://paolacascante.com">Paola Cascante-Bonilla</a><sup>*1,2</sup>, &nbsp; &nbsp;
            <a target="_blank" href="http://web.mit.edu/~shehadak/www/">Khaled Shehada</a><sup>*2,3</sup>, &nbsp; &nbsp;
            <a target="_blank" href="https://jamessealesmith.github.io/">James Seale Smith</a><sup>2,4</sup>, &nbsp; &nbsp;
            <a target="_blank" href="https://scholar.google.com/citations?user=ER4dt8cAAAAJ">Sivan Doveh</a><sup>6,7</sup>, <br>
            <a target="_blank" href="https://cs-people.bu.edu/donhk/">Donghyun Kim</a><sup>2,7</sup>, &nbsp; &nbsp;
            <a target="_blank" href="https://rpand002.github.io/">Rameswar Panda</a><sup>2,7</sup>, &nbsp; &nbsp;
            <a target="_blank" href="https://imagine.enpc.fr/~varolg/">Gül Varol</a><sup>5</sup>, &nbsp; &nbsp;
            <a target="_blank" href="http://olivalab.mit.edu/audeoliva.html">Aude Oliva</a><sup>2,3</sup>, <br> 
            <a target="_blank" href="https://www.cs.rice.edu/~vo9/">Vicente Ordonez</a><sup>1</sup>, &nbsp; &nbsp;
            <a target="_blank" href="https://www.rogerioferis.org/">Rogerio Feris</a><sup>2,7</sup>, &nbsp; &nbsp;
            <a target="_blank" href="https://mitibmwatsonailab.mit.edu/people/leonid-karlinsky/">Leonid Karlinsky</a><sup>2,7</sup>
        </h5>
        <h6 class="header center black-text text-darken-3"><sup>1</sup>Rice University, &nbsp; &nbsp; <sup>2</sup>MIT-IBM Watson AI Lab, &nbsp; &nbsp; 
            <sup>3</sup>MIT, &nbsp; &nbsp; <sup>4</sup>Georgia Institute of Technology, <br>
            <sup>5</sup>LIGM, École des Ponts, &nbsp; &nbsp; <sup>6</sup>Weizmann Institute of Science, &nbsp; &nbsp; <sup>7</sup>IBM Research
        </h6>
        <div class="section">
            <div class="container">
              <div class="row">
                <h6 class="col s12 m1">
                </h6>
                <h5 class="flow-text col s12 m10">
                  <div class="center">
                    <i class="ai ai-obp ai-1x"></i> <a href="https://arxiv.org/abs/2203.17219"><b>Paper</b></a>
                    <!-- &emsp; <i class="ai ai-open-materials ai-1x"></i> <a href=""><b>Dataset [coming soon!]</b></a> -->
                    &emsp; <i class="ai ai-open-materials ai-1x"></i> <a href=""><b>Data & Code</b></a> [soon!]
                    <br><br>
                    <video id="teaser" playsinline="" autoplay="" muted="" loop="" height="350">
                        <source src="syvic_trailer.mp4" type="video/mp4">
                    </video>
                    </div>
                    <br>
                    <br>
                        We investigate to which extent purely synthetic data could be leveraged to teach these models to overcome such shortcomings without compromising their zero-shot 
                        capabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale synthetic dataset and data generation codebase allowing to generate additional 
                        suitable data to improve VLC understanding and compositional reasoning of VL models. Additionally, we propose a general VL finetuning strategy for effectively 
                        leveraging SyViC towards achieving these improvements. Our extensive experiments and ablations on VL-Checklist, Winoground, and ARO benchmarks demonstrate that 
                        it is possible to adapt strong pre-trained VL models with synthetic data significantly enhancing their VLC understanding (e.g. by 9.9% on ARO and 4.3% on VL-Checklist) 
                        with under 1% drop in their zero-shot accuracy.

                  <!-- </div>  -->
                </h5>
              </div>
            </div>
        </div>
    </div>


</body>
</html>
