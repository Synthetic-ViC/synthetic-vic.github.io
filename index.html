<!DOCTYPE html>
<html lang="en" >
<head>
  <meta charset="UTF-8">
  <title>Going Beyond Nouns With Vision & Language Models Using Synthetic Data</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css" media="screen,projection">
  <link rel="stylesheet" href="./style.css">
  <script src="https://code.jquery.com/jquery-3.2.1.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <!-- <script>
      document.addEventListener('DOMContentLoaded', function() {
      var elems = document.querySelectorAll('.materialboxed');
      var instances = M.Materialbox.init(elems, options);
      });
  </script> -->
</head>
<body class="section">
    <div class="section">
        <h3 class="header center black-text text-darken-4"><b>Going Beyond Nouns With Vision & Language</b> <br/><b>Models Using Synthetic Data</b></h3> 
        <h5 class="header center black-text text-darken-3">
            <a target="_blank" href="https://paolacascante.com">Paola Cascante-Bonilla</a><sup>*1,2</sup>, &nbsp; &nbsp;
            <a target="_blank" href="http://web.mit.edu/~shehadak/www/">Khaled Shehada</a><sup>*2,3</sup>, &nbsp; &nbsp;
            <a target="_blank" href="https://jamessealesmith.github.io/">James Seale Smith</a><sup>2,4</sup>, &nbsp; &nbsp;
            <a target="_blank" href="https://scholar.google.com/citations?user=ER4dt8cAAAAJ">Sivan Doveh</a><sup>6,7</sup>, <br>
            <a target="_blank" href="https://cs-people.bu.edu/donhk/">Donghyun Kim</a><sup>2,7</sup>, &nbsp; &nbsp;
            <a target="_blank" href="https://rpand002.github.io/">Rameswar Panda</a><sup>2,7</sup>, &nbsp; &nbsp;
            <a target="_blank" href="https://imagine.enpc.fr/~varolg/">Gül Varol</a><sup>5</sup>, &nbsp; &nbsp;
            <a target="_blank" href="http://olivalab.mit.edu/audeoliva.html">Aude Oliva</a><sup>2,3</sup>, <br> 
            <a target="_blank" href="https://www.cs.rice.edu/~vo9/">Vicente Ordonez</a><sup>1</sup>, &nbsp; &nbsp;
            <a target="_blank" href="https://www.rogerioferis.org/">Rogerio Feris</a><sup>2,7</sup>, &nbsp; &nbsp;
            <a target="_blank" href="https://mitibmwatsonailab.mit.edu/people/leonid-karlinsky/">Leonid Karlinsky</a><sup>2,7</sup>
        </h5>
        <h6 class="header center black-text text-darken-3"><sup>1</sup>Rice University, &nbsp; &nbsp; <sup>2</sup>MIT-IBM Watson AI Lab, &nbsp; &nbsp; 
            <sup>3</sup>MIT, &nbsp; &nbsp; <sup>4</sup>Georgia Institute of Technology, <br>
            <sup>5</sup>LIGM, École des Ponts, &nbsp; &nbsp; <sup>6</sup>Weizmann Institute of Science, &nbsp; &nbsp; <sup>7</sup>IBM Research
        </h6>
        <div class="section">
            <div class="container">
              <div class="row">
                <h6 class="col s12 m1">
                </h6>
                <h5 class="flow-text col s12 m10">
                  <div class="center">
                    <i class="ai ai-obp ai-1x"></i> <a href="https://arxiv.org/abs/2303.17590"><b>Paper</b></a>
                    <!-- &emsp; <i class="ai ai-open-materials ai-1x"></i> <a href=""><b>Dataset [coming soon!]</b></a> -->
                    &emsp; <i class="ai ai-open-materials ai-1x"></i> <a href="https://github.com/uvavision/SyViC"><b>Data & Code</b></a>
                    <br><br>
                    <video id="teaser" playsinline="" autoplay="" muted="" loop="" height="350">
                        <source src="syvic_trailer.mp4" type="video/mp4">
                    </video>
                    </div>
                    <!-- <br>
                    <br>
                        We investigate to which extent purely synthetic data could be leveraged to teach these models to overcome such shortcomings without compromising their zero-shot 
                        capabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale synthetic dataset and data generation codebase allowing to generate additional 
                        suitable data to improve VLC understanding and compositional reasoning of VL models. Additionally, we propose a general VL finetuning strategy for effectively 
                        leveraging SyViC towards achieving these improvements. Our extensive experiments and ablations on VL-Checklist, Winoground, and ARO benchmarks demonstrate that 
                        it is possible to adapt strong pre-trained VL models with synthetic data significantly enhancing their VLC understanding (e.g. by 9.9% on ARO and 4.3% on VL-Checklist) 
                        with under 1% drop in their zero-shot accuracy. -->
                  <!-- </div>  -->
                </h5>
                <div class="col s2 m2 l2"></div>
                <div class="col s8 m8 l8">
                    <br>
                    <h5 class="center"><b>Abstract</b></h5>
                    <div class="divider"></div>
                    <p>
                        Large-scale pre-trained Vision & Language (VL) models have shown remarkable performance in many applications, enabling replacing a fixed set of supported classes 
                        with zero-shot open vocabulary reasoning over (almost arbitrary) natural language prompts. However, recent works have uncovered a fundamental weakness of these models. 
                        For example, their difficulty to understand Visual Language Concepts (VLC) that go 'beyond nouns' such as the meaning of non-object words (e.g., attributes, actions, 
                        relations, states, etc.), or difficulty in performing compositional reasoning such as understanding the significance of the order of the words in a sentence. In this work,
                         we investigate to which extent purely synthetic data could be leveraged to teach these models to overcome such shortcomings without compromising their zero-shot capabilities. 
                         We contribute Synthetic Visual Concepts (SyViC) - a million-scale synthetic dataset and data generation codebase allowing to generate additional suitable data to improve VLC 
                         understanding and compositional reasoning of VL models. Additionally, we propose a general VL finetuning strategy for effectively leveraging SyViC towards achieving these 
                         improvements. Our extensive experiments and ablations on VL-Checklist, Winoground, and ARO benchmarks demonstrate that it is possible to adapt strong pre-trained VL models 
                         with synthetic data significantly enhancing their VLC understanding (e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under 1% drop in their zero-shot accuracy.
                   </p>
                   <div class="divider"></div>
                   <br>
               </div>
               <div class="col s2 m2 l2"></div>
               <div class="row">
                <div class="col s12">
                    <h5 class="center"><b>Overview</b></h5><br>
                    <h6>We finetune a large-scale pre-trained VL model using our <b class="blue-text text-darken-2">SyViC</b> dataset. Our proposed methodology includes domain adaptation and avoiding forgetting through parameterefficient finetuning (LoRA) and model averaging.
                        <br><br>
                    <p class="center">
                        <img class="responsive-img" src="overview.png" width="80%">
                    </p>
                </div>
                <div class="col s2 m2 l2"></div>
                <div class="col s8 m8 l8">
                    <h6 class="center">Summary of the entire flow, including components and choices of <b class="blue-text text-darken-2">SyViC</b> data synthesis
                    pipeline (left) and the proposed effective finetuning technique (right).</h6>
                </div>
                <div class="col s2 m2 l2"></div>
               </div>

               <!-- //////////////////////////// -->

               <div class="row">
                <div class="col s12">
                    <h5 class="center"><b>Results</b></h5><br>
                    <h6>We evaluate our finetuned model on three benchmarks:
<i>VL-Checklist</i>, <i>ARO</i>, and <i>Winoground</i>. The <u>compositional
reasoning evaluation</u> includes understanding the
meaning of the sentence after changing the <b>word
order</b>, <b>attributes</b>, and <b>relations</b> of humans/objects.</h6><br>
                    <p class="center">
                        <img class="responsive-img" src="SomeResults.png" width="80%">
                    </p>
                </div>
                <div class="col s2 m2 l2"></div>
               </div>

               <!-- //////////////////////////// -->

               <div class="row">
                <div class="col s12">
                    <h5 class="center"><b>Poster</b></h5><br>
                    <p class="center">
                        <object data="ICCV_GoingBeyondNouns_60x40.pdf" width="710px" height="530px"></object>
                    </p>
                </div>
                <div class="col s2 m2 l2"></div>
               </div>

               <!-- //////////////////////////// -->

               <div class="row">
                <div class="col s2 m2 l2"></div>
                <div class="col s8 m8 l8">
                    <br>
                    <br>
                    <br>
                    <div class="divider"></div>
                    <h6 class="center"><b>BibTeX</b></h6>
                    <blockquote>
                    <font face="Courier New">
                        @misc{cascantebonilla2023going, <br>
                            &nbsp; &nbsp; title={Going Beyond Nouns With Vision & Language Models Using Synthetic Data}, <br>
                            &nbsp; &nbsp; author={Paola Cascante-Bonilla, Khaled Shehada, James Seale Smith, Sivan Doveh, Donghyun Kim, Rameswar Panda, Gül Varol, Aude Oliva, Vicente Ordonez, Rogerio Feris, Leonid Karlinsky}, <br>
                            &nbsp; &nbsp; year={2023}, <br>
                            &nbsp; &nbsp; eprint={2303.17590}, <br>
                            &nbsp; &nbsp; archivePrefix={arXiv}, <br>
                            &nbsp; &nbsp; primaryClass={cs.CV}
                        }
                    </font>
                    </blockquote>
                </div>
                <div class="col s2 m2 l2"></div>
              </div>
            </div>
        </div>
    </div>

    


</body>
</html>
